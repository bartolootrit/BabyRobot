import numpy as np
from grid_level import GridLevel

''' implement the Value Iteration algorithm '''


class ValueIteration():
    policy = None

    def __init__(self, level, discount_factor=0.9):
        self.level = level
        self.values = np.zeros((level.height, level.width))
        self.discount_factor = discount_factor

    def get_state_value(self, pos):
        ''' get the currently calculated value of the specified position in the grid '''
        x = pos[0]
        y = pos[1]
        if (x < 0 or x >= self.level.width) or (y < 0 or y >= self.level.height): return 0
        return self.values[y, x]

    def calculate_max_action_value(self, x, y):
        '''
            calculate the values of all actions in the specified cell and return the largest of these
            - the next state value is given by:  v(s) = max[r + Î³v(s')]
        '''

        # check that some actions are possible in this state
        policy_actions = self.level.get_available_actions(x, y, self.policy)
        if not policy_actions: return 0

        # calculate the value of each action in the state and save the largest
        max_value = float('-inf')
        for policy_direction, policy_v in policy_actions.items():
            # test the action is allowed
            if policy_v == True:

                # get the list of all other possible states
                all_actions = self.level.get_available_actions(x, y)

                # count the number of states other than the target state
                num_alternative_states = sum(all_actions.values()) - 1

                # get the probability of moving to the intended target
                transition_probability = self.level.get_transition_probability(x, y)

                # if there are no alternative states then the probability of moving to the target is 1
                if num_alternative_states == 0:
                    transition_probability = 1.

                # sum the rewards and values of the possible next states
                value = 0
                for direction, v in all_actions.items():
                    # test the action is allowed
                    if v == True:

                        # calculate the probability of moving to this state
                        if direction == policy_direction:
                            probability = transition_probability
                        else:
                            probability = (1 - transition_probability) / num_alternative_states

                        # calculate the postion of the next state
                        next_pos = self.level.get_next_state_position(x, y, direction)

                        # get the reward for taking this action
                        reward = self.level.get_action_reward(next_pos[0], next_pos[1])

                        # combine the reward with discounted value of the next state and
                        # sum over each of the transition probabilities p(s'|s,a)
                        value += probability * (
                                reward + (self.discount_factor * self.get_state_value(next_pos)))

                        # save the largest value
                if value > max_value:
                    max_value = value

                    # return the value of the largest action-value in this state
        return max_value

    def state_sweep(self):
        ''' calculate the value of all states except the exit '''

        new_values = np.zeros((self.level.height, self.level.width))
        end = self.level.get_end()
        for y in range(self.level.height):
            for x in range(self.level.width):
                if (x != end[0]) or (y != end[1]):
                    new_values[y, x] = self.calculate_max_action_value(x, y)

                    # calculate the largest difference in the state values between the start and end of the sweep
        delta = np.max(np.abs(new_values - self.values))

        # update the state values with the newly calculated values
        self.values = new_values

        # return the largest state value difference
        return delta

    def run_to_convergence(self, max_iterations=100, threshold=1e-3):
        ''' run multiple state sweeps until the maximum change in the state value falls
            below the supplied threshold or the maximum number of iterations is reached
        '''

        for n in range(max_iterations):

            # calculate the maximum action value in each state and get the largest state value difference
            delta = self.state_sweep()

            # test if the difference is less than the defined convergence threshold
            if delta < threshold:
                break

        # return the number of iterations taken to converge
        return n
